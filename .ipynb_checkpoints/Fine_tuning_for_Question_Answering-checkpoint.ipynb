{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/Heimine/NLU_project/blob/Yichen-Liu/Fine_tuning_for_Question_Answering.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1qfQAtRsMVl7"
   },
   "source": [
    "# Fine_tuning_for_Question_Answering\n",
    "\n",
    "Origin: @techno246\n",
    "\n",
    "Adaptation: Yichen Liu\n",
    "\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Reading comprehension, otherwise known as question answering systems, are one of the tasks that NLP tries to solve. The goal of this task is to be able to answer an arbitary question given a context. In our project, we want to build a QA model that can automatically search answer from given context. For instance, given the following context:\n",
    "\n",
    "> Coronavirus disease 2019 (COVID-19) is an infectious disease caused by severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2). The disease was first identified in December 2019 in Wuhan, the capital of China's Hubei province, and has since spread globally, resulting in the ongoing 2019â€“20 coronavirus pandemic. As of 1 May 2020, more than 3.27 million cases have been reported across 187 countries and territories, resulting in more than 233,000 deaths. More than 1.02 million people have recovered.\n",
    "\n",
    "We ask the question\n",
    "\n",
    "> How many cases have been reported as of 1 May 2020?\n",
    "\n",
    "We expect the QA system is to respond with something like this:\n",
    "\n",
    "> more than 3.27 million\n",
    "\n",
    "\n",
    "This notebook demonstrates how we fine-tune ALBERT for the task of QnA and use it for inference. For this tutorial, we will use the transformer library built by [Hugging Face](https://huggingface.co/), which is an extremely nice implementation of the transformer models (including ALBERT) in both TensorFlow and PyTorch. You can  just use a fine-tuned model from their [model repository](https://huggingface.co/models) (which I encourage in general to save money and reduce emissions). However in our project, we modify some codes in original transformer library to load our hand-made testset. The modified version can be acceess [here](https://github.com/lyc1005/transformers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sBBHbGvQN5vX"
   },
   "source": [
    "## 1.0 Setup\n",
    "\n",
    "Let's check out what kind of GPU our friends at Google gave us. This notebook should be configured to give you a P100 ðŸ˜ƒ (saved in metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "frTeTcy4WdbY"
   },
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D5RImM3oWbrZ"
   },
   "source": [
    "First, we clone the Hugging Face transformer library from [here](https://github.com/lyc1005/transformers)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QOAoUwBFMQCg"
   },
   "outputs": [],
   "source": [
    "#!git clone https://github.com/huggingface/transformers \\\n",
    "!git clone https://github.com/lyc1005/transformers.git \\\n",
    "&& cd transformers \\\n",
    "#&& git checkout a3085020ed0d81d4903c50967687192e3101e770 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TRZned-8WJrj"
   },
   "outputs": [],
   "source": [
    "!pip install ./transformers\n",
    "!pip install tensorboardX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UHCuzhPptH0M"
   },
   "source": [
    "## 2.0 Train Model\n",
    "\n",
    "This is where we can train our own model. Note you can skip this step if you don't want to wait 1.5 hours!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OaQGsAiWXcnd"
   },
   "source": [
    "### 2.1 Get Training and Evaluation Data\n",
    "\n",
    "The SQuAD dataset contains question/answer pairs to for training the ALBERT model for the QA task. \n",
    "\n",
    "Now get the SQuAD V2.0 dataset. `train-v2.0.json` is for training and `dev-v2.0.json` is for evaluation to see how well your model trained.\n",
    "\n",
    "Read more about this dataset here: https://rajpurkar.github.io/SQuAD-explorer/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dI6e-PfOXSnO"
   },
   "outputs": [],
   "source": [
    "!mkdir SQuAD2.0 \\\n",
    "&& cd SQuAD2.0 \\\n",
    "&& wget https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v2.0.json \\\n",
    "&& wget https://rajpurkar.github.io/SQuAD-explorer/dataset/dev-v2.0.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hhDCVxsgx0YL"
   },
   "source": [
    "### Download the test data set which is in our github repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-C_oNLD5FnFU"
   },
   "outputs": [],
   "source": [
    "!git clone https://github.com/Heimine/NLU_project.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cNBSQxTeIbW0"
   },
   "source": [
    "### Convert TF-based Biobert to Pytorch model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ddHmP2k6Gw5H"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-8mCJRIKK7Mq"
   },
   "outputs": [],
   "source": [
    "!tar -xzf \"/content/drive/My Drive/biobert_v1.1_pubmed.tar.gz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_D8J-N4LwoCT"
   },
   "outputs": [],
   "source": [
    "!pip install pytorch-transformers\n",
    "!pytorch_transformers bert biobert_v1.1_pubmed/model.ckpt-1000000 biobert_v1.1_pubmed/bert_config.json biobert_v1.1_pubmed/pytorch_model.bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "X8ulX_GrIMli"
   },
   "outputs": [],
   "source": [
    "!mv biobert_v1.1_pubmed/bert_config.json biobert_v1.1_pubmed/config.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Jk2jpvNA3syR"
   },
   "source": [
    "### Convert TF-based CovidAlbert to Pytorch model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gVgd8gBP3sKx"
   },
   "outputs": [],
   "source": [
    "!unzip \"/content/drive/My Drive/Covid-Albert.zip\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_GDf_Mx8X36Q"
   },
   "outputs": [],
   "source": [
    "!python /content/transformers/src/transformers/convert_albert_original_tf_checkpoint_to_pytorch.py \\\n",
    "    --tf_checkpoint_path=/content/Covid-Albert/albert_100000_check/train_output_model.ckpt-100000 \\\n",
    "    --albert_config_file=/content/Covid-Albert/albert_100000_check/albert_config.json \\\n",
    "    --pytorch_dump_path=/content/Covid-Albert/albert_100000_check/pytorch_model.bin \\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lzXerg7kZiTV"
   },
   "outputs": [],
   "source": [
    "!mv /content/Covid-Albert/albert_100000_check/albert_config.json /content/Covid-Albert/albert_100000_check/config.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iMpZFHhglfVY"
   },
   "outputs": [],
   "source": [
    "!mv /content/Covid-Albert/albert_100000_check/30k-clean.model /content/Covid-Albert/albert_100000_check/spiece.model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dZ87q93GDeeL"
   },
   "source": [
    "### 2.2 Run training \n",
    "\n",
    "We can now train the model with the training set. \n",
    "\n",
    "### Notes about parameters:\n",
    "`per_gpu_train_batch_size` specifies the number of training examples per iteration per GPU. *In general*, higher means more accuracy and faster training. However, the biggest limitation is the size of the GPU. 12 is what I use for a GPU with 16GB memory. \n",
    "\n",
    "`save_steps` specifies number of steps before it outputs a checkpoint file. I've increased it to save disk space.\n",
    "\n",
    "`num_train_epochs` I recommend two epochs here. It's currently set to one for the purpose of time\n",
    "\n",
    "`version_2_with_negative` is required for SQuAD V2.0. If training with V1.1, take out this flag\n",
    "\n",
    "Warning: it takes about 1.5 hours to train an epoch! If you don't want to wait this long, feel free to skip this step and note the comment in the code to use a pretrained model!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0T_S6vsmUX-m"
   },
   "source": [
    "### Fine tuning Albert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PQPILxE9xx75"
   },
   "outputs": [],
   "source": [
    "# Albert\n",
    "# after fune tune on SQuAD 2.0\n",
    "{'exact': 63.5, 'f1': 83.9611555413186, 'total': 200, 'HasAns_exact': 62.63157894736842, 'HasAns_f1': 84.16963741191432, 'HasAns_total': 190, 'NoAns_exact': 80.0, 'NoAns_f1': 80.0, 'NoAns_total': 10, 'best_exact': 63.5, 'best_exact_thresh': 0.0, 'best_f1': 83.96115554131856, 'best_f1_thresh': 0.0}\n",
    "# after fune tune on SQuAD and BioASQ 2\n",
    "{'exact': 48.5, 'f1': 72.63644087813776, 'total': 200, 'HasAns_exact': 47.89473684210526, 'HasAns_f1': 73.30151671382926, 'HasAns_total': 190, 'NoAns_exact': 60.0, 'NoAns_f1': 60.0, 'NoAns_total': 10, 'best_exact': 48.5, 'best_exact_thresh': 0.0, 'best_f1': 72.63644087813778, 'best_f1_thresh': 0.0}\n",
    "# after fune tune on SQuAD and BioASQ 1\n",
    "{'exact': 48.0, 'f1': 72.69904379246483, 'total': 200, 'HasAns_exact': 47.36842105263158, 'HasAns_f1': 73.36741451838407, 'HasAns_total': 190, 'NoAns_exact': 60.0, 'NoAns_f1': 60.0, 'NoAns_total': 10, 'best_exact': 48.0, 'best_exact_thresh': 0.0, 'best_f1': 72.69904379246483, 'best_f1_thresh': 0.0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-Eg53t3QXZAb"
   },
   "outputs": [],
   "source": [
    "!python transformers/examples/run_squad.py \\\n",
    "  --model_type albert \\\n",
    "  --model_name_or_path albert-base-v2 \\\n",
    "  --do_train \\\n",
    "  --do_eval \\\n",
    "  --do_lower_case \\\n",
    "  --train_file /content/SQuAD2.0/train-v2.0.json \\\n",
    "  --predict_file /content/NLU_project/COVID19_QA_testset.csv \\\n",
    "  --per_gpu_train_batch_size 12 \\\n",
    "  --learning_rate 3e-5 \\\n",
    "  --num_train_epochs 1 \\\n",
    "  --max_seq_length 384 \\\n",
    "  --doc_stride 128 \\\n",
    "  --output_dir /content/SQuAD_Albert_model \\\n",
    "  --save_steps 1000 \\\n",
    "  --threads 4 \\\n",
    "  --version_2_with_negative"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NYoNDsNEUnep"
   },
   "source": [
    "### Fine tuning Biobert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "X0ivEiQ6syxI"
   },
   "outputs": [],
   "source": [
    "# Biobert\n",
    "# after fune tune on SQuAD 2.0\n",
    "{'exact': 64.5, 'f1': 83.89901381595338, 'total': 200, 'HasAns_exact': 63.1578947368421, 'HasAns_f1': 83.57790927995093, 'HasAns_total': 190, 'NoAns_exact': 90.0, 'NoAns_f1': 90.0, 'NoAns_total': 10, 'best_exact': 64.5, 'best_exact_thresh': 0.0, 'best_f1': 83.89901381595337, 'best_f1_thresh': 0.0}\n",
    "# after fune tune on SQuAD and BioASQ 2\n",
    "{'exact': 47.5, 'f1': 67.90449716949715, 'total': 200, 'HasAns_exact': 45.78947368421053, 'HasAns_f1': 67.26789175736543, 'HasAns_total': 190, 'NoAns_exact': 80.0, 'NoAns_f1': 80.0, 'NoAns_total': 10, 'best_exact': 47.5, 'best_exact_thresh': 0.0, 'best_f1': 67.90449716949716, 'best_f1_thresh': 0.0}\n",
    "# after fune tune on SQuAD and BioASQ 1\n",
    "{'exact': 48.5, 'f1': 70.26100288600288, 'total': 200, 'HasAns_exact': 46.8421052631579, 'HasAns_f1': 69.74842409052934, 'HasAns_total': 190, 'NoAns_exact': 80.0, 'NoAns_f1': 80.0, 'NoAns_total': 10, 'best_exact': 48.5, 'best_exact_thresh': 0.0, 'best_f1': 70.26100288600287, 'best_f1_thresh': 0.0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cmvG9toyUtl9"
   },
   "outputs": [],
   "source": [
    "!python transformers/examples/run_squad.py \\\n",
    "  --model_type bert \\\n",
    "  --model_name_or_path /content/biobert_v1.1_pubmed \\\n",
    "  --do_train \\\n",
    "  --do_eval \\\n",
    "  --do_lower_case \\\n",
    "  --train_file /content/NLU_project/BioASQ-7b/train/Full-Abstract/BioASQ-train-factoid-7b-full-annotated.json \\\n",
    "  --predict_file /content/NLU_project/COVID19_QA_testset.csv \\\n",
    "  --per_gpu_train_batch_size 12 \\\n",
    "  --learning_rate 3e-5 \\\n",
    "  --num_train_epochs 1 \\\n",
    "  --max_seq_length 384 \\\n",
    "  --doc_stride 128 \\\n",
    "  --output_dir /content/SQuAD_Biobert_model \\\n",
    "  --save_steps 1000 \\\n",
    "  --threads 4 \\\n",
    "  --version_2_with_negative"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CR35RQjD2eiS"
   },
   "source": [
    "### Fine tuning CovidAlbert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2rLrMdvT9jz2"
   },
   "outputs": [],
   "source": [
    "# CovidAlbert\n",
    "# after fune tune on SQuAD 2.0\n",
    "\n",
    "# after fune tune on SQuAD and BioASQ 2\n",
    "\n",
    "# after fune tune on SQuAD and BioASQ 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "G88UQkATHgbJ"
   },
   "outputs": [],
   "source": [
    "!python transformers/examples/run_squad.py \\\n",
    "  --model_type albert \\\n",
    "  --model_name_or_path /content/Covid-Albert/albert_100000_check \\\n",
    "  --do_train \\\n",
    "  --do_eval \\\n",
    "  --do_lower_case \\\n",
    "  --train_file /content/SQuAD2.0/train-v2.0.json \\\n",
    "  --predict_file /content/NLU_project/COVID19_QA_testset.csv \\\n",
    "  --per_gpu_train_batch_size 12 \\\n",
    "  --learning_rate 3e-5 \\\n",
    "  --num_train_epochs 1 \\\n",
    "  --max_seq_length 384 \\\n",
    "  --doc_stride 128 \\\n",
    "  --output_dir /content/SQuAD_CovidAlbert_model \\\n",
    "  --save_steps 5000 \\\n",
    "  --threads 4 \\\n",
    "  --version_2_with_negative"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-JCNRkQwUD56"
   },
   "source": [
    "## 3.0 Setup prediction code\n",
    "\n",
    "Now we can use the Hugging Face library to make predictions using our newly trained model. Note that a lot of the code is pulled from `run_squad.py` in the Hugging Face repository, with all the training parts removed. This modified code allows to run predictions we pass in directly as strings, rather .json format like the training/test set.\n",
    "\n",
    "NOTE if you decided train your own mode, change the flag `use_own_model` to `True`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "cellView": "code",
    "colab": {},
    "colab_type": "code",
    "id": "qp0Pq9z9Y4S0"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import time\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "from transformers import (\n",
    "    AlbertConfig,\n",
    "    AlbertForQuestionAnswering,\n",
    "    AlbertTokenizer,\n",
    "    squad_convert_examples_to_features\n",
    ")\n",
    "\n",
    "from transformers.data.processors.squad import SquadResult, SquadV2Processor, SquadExample\n",
    "\n",
    "from transformers.data.metrics.squad_metrics import compute_predictions_logits\n",
    "\n",
    "# READER NOTE: Set this flag to use own model, or use pretrained model in the Hugging Face repository\n",
    "use_own_model = True\n",
    "\n",
    "if use_own_model:\n",
    "  model_name_or_path = \"/content/SQuAD_Biobert_model\"\n",
    "else:\n",
    "  model_name_or_path = \"ktrapeznikov/albert-xlarge-v2-squad-v2\"\n",
    "\n",
    "output_dir = \"\"\n",
    "\n",
    "# Config\n",
    "n_best_size = 1\n",
    "max_answer_length = 30\n",
    "do_lower_case = True\n",
    "null_score_diff_threshold = 0.0\n",
    "\n",
    "def to_list(tensor):\n",
    "    return tensor.detach().cpu().tolist()\n",
    "\n",
    "# Setup model\n",
    "config_class, model_class, tokenizer_class = (\n",
    "    AlbertConfig, AlbertForQuestionAnswering, AlbertTokenizer)\n",
    "config = config_class.from_pretrained(model_name_or_path)\n",
    "tokenizer = tokenizer_class.from_pretrained(\n",
    "    model_name_or_path, do_lower_case=True)\n",
    "model = model_class.from_pretrained(model_name_or_path, config=config)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "processor = SquadV2Processor()\n",
    "\n",
    "def run_prediction(question_texts, context_text):\n",
    "    \"\"\"Setup function to compute predictions\"\"\"\n",
    "    examples = []\n",
    "\n",
    "    for i, question_text in enumerate(question_texts):\n",
    "        example = SquadExample(\n",
    "            qas_id=str(i),\n",
    "            question_text=question_text,\n",
    "            context_text=context_text,\n",
    "            answer_text=None,\n",
    "            start_position_character=None,\n",
    "            title=\"Predict\",\n",
    "            is_impossible=False,\n",
    "            answers=None,\n",
    "        )\n",
    "\n",
    "        examples.append(example)\n",
    "\n",
    "    features, dataset = squad_convert_examples_to_features(\n",
    "        examples=examples,\n",
    "        tokenizer=tokenizer,\n",
    "        max_seq_length=384,\n",
    "        doc_stride=128,\n",
    "        max_query_length=64,\n",
    "        is_training=False,\n",
    "        return_dataset=\"pt\",\n",
    "        threads=1,\n",
    "    )\n",
    "\n",
    "    eval_sampler = SequentialSampler(dataset)\n",
    "    eval_dataloader = DataLoader(dataset, sampler=eval_sampler, batch_size=10)\n",
    "\n",
    "    all_results = []\n",
    "\n",
    "    for batch in eval_dataloader:\n",
    "        model.eval()\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            inputs = {\n",
    "                \"input_ids\": batch[0],\n",
    "                \"attention_mask\": batch[1],\n",
    "                \"token_type_ids\": batch[2],\n",
    "            }\n",
    "\n",
    "            example_indices = batch[3]\n",
    "\n",
    "            outputs = model(**inputs)\n",
    "\n",
    "            for i, example_index in enumerate(example_indices):\n",
    "                eval_feature = features[example_index.item()]\n",
    "                unique_id = int(eval_feature.unique_id)\n",
    "\n",
    "                output = [to_list(output[i]) for output in outputs]\n",
    "\n",
    "                start_logits, end_logits = output\n",
    "                result = SquadResult(unique_id, start_logits, end_logits)\n",
    "                all_results.append(result)\n",
    "\n",
    "    output_prediction_file = \"predictions.json\"\n",
    "    output_nbest_file = \"nbest_predictions.json\"\n",
    "    output_null_log_odds_file = \"null_predictions.json\"\n",
    "\n",
    "    predictions = compute_predictions_logits(\n",
    "        examples,\n",
    "        features,\n",
    "        all_results,\n",
    "        n_best_size,\n",
    "        max_answer_length,\n",
    "        do_lower_case,\n",
    "        output_prediction_file,\n",
    "        output_nbest_file,\n",
    "        output_null_log_odds_file,\n",
    "        False,  # verbose_logging\n",
    "        True,  # version_2_with_negative\n",
    "        null_score_diff_threshold,\n",
    "        tokenizer,\n",
    "    )\n",
    "\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nIQOB8vhpcKs"
   },
   "source": [
    "## 4.0 Run predictions\n",
    "\n",
    "Now for the fun part... testing out your model on different inputs. Pretty rudimentary example here. But the possibilities are endless with this function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "cellView": "code",
    "colab": {},
    "colab_type": "code",
    "id": "F-sUrcA5nXTH"
   },
   "outputs": [],
   "source": [
    "context = \"Coronavirus disease 2019 (COVID-19) is an infectious disease caused by severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2). The disease was first identified in December 2019 in Wuhan, the capital of China's Hubei province, and has since spread globally, resulting in the ongoing 2019â€“20 coronavirus pandemic. As of 1 May 2020, more than 3.27 million cases have been reported across 187 countries and territories, resulting in more than 233,000 deaths. More than 1.02 million people have recovered.\"\n",
    "questions = [\"Where did COVID-19 originate from\",              \n",
    "             \"How many cases have been reported as of 1 May 2020\",\n",
    "             \"How many people have died from COVID-19\",\n",
    "             \"Which country suffers most from COVID-19\"]\n",
    "\n",
    "predictions = run_prediction(questions, context)\n",
    "\n",
    "# Print results\n",
    "for key in predictions.keys():\n",
    "  print(predictions[key])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TUxj7vMrujRL"
   },
   "source": [
    "# Store the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-4mKRg2duTuQ"
   },
   "outputs": [],
   "source": [
    "#!zip -r model_output_squad.zip model_output_squad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MNse0uSVAMdV"
   },
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "files.download(\"/content/SQuAD_Biobert_model/pytorch_model.bin\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "machine_shape": "hm",
   "name": "Fine_tuning for Question Answering.ipynb",
   "private_outputs": true,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
